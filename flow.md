# Botâ€‘Influencer Architecture & Data Flywheel

This document captures the full technical plan for a selfâ€‘learning, personalityâ€‘driven AI agent that monitors Twitter and Telegram for RWAâ€‘gold & crypto chatter, filters noise, learns continuously, and publishes posts/comments under its own brand voice.  
It combines a **dataâ€‘flywheel** with Retrievalâ€‘Augmented Generation (RAG), daily LoRA microâ€‘tuning, and a behaviour policy trained with reinforcement learning from engagement signals.

---

## ðŸ”„ Highâ€‘Level Flow Diagram

```mermaid
flowchart TD
    %% ====  SOURCES  ====
    A(Twitter<br/>Stream API) -->|raw JSON| FQ(Data&nbsp;QualityÂ Gate)
    B(Telegram<br/>Public Chats) -->|raw messages| FQ
    C(Internal<br/>KnowledgeÂ KG) --> IDX
    %% ====  QUALITY GATE  ====
    FQ -->|clean<br/>chunks| EMB(Embed<br/>â†’ vectors)
    FQ -->|drop spam<br/>toxicity| TRASH{{Discard}}

    %% ====  RETRIEVAL LOOP ====
    EMB -->|HNSW / IVFâ€‘PQ| IDX[Vector DB<br/>(Pinecone/Qdrant)]
    IDX -->|topâ€‘k IDs| RERANK(Crossâ€‘encoder<br/>reâ€‘rank)

    %% ====  GENERATION LOOP ====
    RERANK -->|context| GEN(LLM +<br/>LoRA adapters)
    GEN -->|draft+critique| SELF(Selfâ€‘RAG & Reflexion)
    SELF -->|personaâ€‘consistent<br/>reply| POST(Post to<br/>Twitter/Telegram)

    %% ====  BEHAVIOUR LOOP ====
    POST -->|engagement<br/>(likes RTÂ clicks)| REWARD(RewardÂ Store)
    REWARD -->|weekly PPO| POLICY(BehaviourÂ Policy)
    POLICY -->|reading &<br/>posting decisions| A
    POLICY -->|reading &<br/>posting decisions| B

    %% ====  METRICS & OPS ====
    IDX -. metrics .-> MON[Dashboard<br/>(RAGAS/Prom)]
    GEN -. faithfulness .-> MON
    POST -. brand drift .-> MON
```

---

## 1. Dataâ€‘Quality Gate (LoopÂ 1)

| Check | Method | Threshold | Action |
|-------|--------|-----------|--------|
| Language | `fastText` langâ€‘ID | nonâ€‘English? | filter |
| Toxicity | Google Perspective | >Â 0.80 | discard |
| Bot score | Botometerâ€‘Lite | topÂ 10â€¯% | discard |
| Perplexity band | GPTâ€‘2 PPL | keep 10â€‘90â€¯% | keep |
| Engagement | likesâ€¯+â€¯RT above median | âœ“ | priority |

Only messages passing **all** checks are chunked (256â€¯tokens) and embedded.

---

## 2. Retrieval Loop (LoopÂ 2)

* **Embedding model:** `bgeâ€‘largeâ€‘en` (or `textâ€‘embeddingâ€‘3â€‘small` if using OpenAI).  
* **Index:** HNSW <â€¯10â€¯M vectors; migrate to IVFâ€‘PQ + GPU search above that.  
* **Reâ€‘ranking:** ColBERTâ€‘v2 crossâ€‘encoder adds ~10â€“15â€¯% precision.  
* **Key metrics:** precision@5, recall@10, contextâ€‘precision (RAGAS).  
* **Trigger:** reâ€‘index when precision@5 drops by 5â€¯% WoW.

---

## 3. Generation Loop (LoopÂ 3)

| Hyperâ€‘param | Value | Note |
|-------------|-------|------|
| Base model | Mistralâ€¯7B or Llamaâ€‘3â€¯8B | openâ€‘weights |
| LoRA rank `r` | 16 | qualityâ€¯/â€¯VRAM tradeâ€‘off |
| Alpha | 2â€¯Ã—â€¯r | scaling rule |
| LR (AdamW) | 1â€¯Ã—â€¯10â»â´ | tune first |
| Epochs | 1 | avoid overâ€‘fit |

Daily microâ€‘adapters are merged back every 4â€“6â€¯weeks to prevent adapter sprawl.

**Selfâ€‘RAG + Reflexion**: model critiques and iterates once before final post; cuts hallucinations ~30â€¯%.

---

## 4. Behaviour Loop (LoopÂ 4)

* **Reward model inputs:** likes, retweets, CTR, follower delta (positive); toxicity, offâ€‘topic, low faithfulness (negative).  
* **Policy learner:** PPO updated weekly with 1â€‘step importance sampling.  
* **Safety valve:** if faithfulnessÂ <â€¯0.9 or toxicityÂ >â€¯0.6, autoâ€‘block posting & alert human.

---

## 5. Daily Cadence (SGT)

| Time | Job | Loop |
|------|-----|------|
| 01:00 | Hydrate 24â€¯h sources â†’ run quality gate | 1 |
| 02:00 | Embed & upsert vectors; rebuild ANN & reâ€‘rank index | 2 |
| 03:00 | LoRA fineâ€‘tune on new highâ€‘quality chunks | 3 |
| 09â€¯â€“â€¯23â€¯h | Agent reads, answers, posts (Selfâ€‘RAG active) | 2â€‘3 |
| 23:30 | Aggregate metrics â†’ reward logs â†’ PPOÂ update | 4 |

---

## 6. Goldenâ€‘Signal Dashboard

| Signal | Threshold | Action |
|--------|-----------|--------|
| Retrieval precision@5 | â†“â€¯>â€¯5â€¯% WoW | reâ€‘index |
| Faithfulness | <â€¯0.90 | tighten filters / retrain generator |
| Hallucination rate | â†‘â€¯WoW | increase Selfâ€‘RAG passes |
| p95 latency | >â€¯2â€¯s | scale index / lower k |
| Adapter count | >â€¯8 | merge weights |

---

## 7. Knowledgeâ€‘Graph Hybrid

Immutable data (e.g., LBMA rules) lives in a Neo4j KG.  
Retriever first queries KG; if miss, fall back to vector DB, ensuring critical facts never drift.

---

## 8. Future Enhancements

* **Multiâ€‘lingual switch**: add languageâ€‘specific adapters & embeddings.  
* **Onâ€‘device summariser**: distil daily streams into trend reports.  
* **Synthetic user simulator**: generate interaction traces to preâ€‘train behaviour policy before launch.

---

