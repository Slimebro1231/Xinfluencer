# Botâ€‘Influencer Architecture & Data Flywheel

At launch the agent is given a seed list of ~100 trusted KOL accounts. Humans watch its first outputs (the bootstrap gate) while automated filters kill spam, bots and toxic tweets. Clean chunks go into a GPU-backed vector DB. At inference the model uses Self-RAGâ€”retrieve â†’ draft â†’ re-retrieve & critiqueâ€”to write tweets with a certain personality. Every draft is scored three ways: (1) humans, (2) an â€œAI peerâ€ critic running raw GPT-4o, and (3) real Twitter engagement. All three signals feed a reward model; once a week a PPO policy update (via the Hugging-Face TRL library) shifts what the bot reads and how it speaks. Meanwhile a daily LoRA micro-tune nudges the LLM itself. A Prometheus + Grafana dashboard plus the RAGAS evaluation library surface retrieval precision, faithfulness and latency so ops can see drift in real time. LoRA's key knobs are the rankâ€¯*r*â€¯(we useâ€¯16) and scaling factorâ€¯Î±â€¯(â‰ˆâ€¯2â€¯Ã—â€¯*r*); they decide how many new parameters the adapter learns (~2â€¯% of the model) and how strongly they steer the frozen weights, while **RAGAS** (Retrievalâ€‘Augmentedâ€‘Generation Assessment Suite) tracks contextâ€‘precision, faithfulness, answerâ€‘relevancy and latency so we spot drift early.


## ğŸ”„ Highâ€‘Level Flow Diagram

```mermaid
flowchart TD
    %% ========== INGESTION ==========
    KOL["Seed KOL List<br/>(~100 experts)"] --> TWAPI("Twitter API<br/>(Tweepy)")
    TWAPI --> RAW["Raw Tweets<br/>(JSON)"]

    %% ========== DATA QUALITY & HUMAN ALIGNMENT ==========
    RAW --> QC["Automated Filters<br/>(lang âœ”, toxicity âœ”,<br/>bot âœ”, perplexity âœ”)"]
    QC --> HREV{"Human Align<br/>Review (bootstrap)"}
    HREV -- "approve" --> CHUNK["Cleaned<br/>256â€‘token chunks"]
    HREV -- "reject" --> DISCARD["Discard"]

    %% ========== VECTOR & RAG ==========
    CHUNK --> EMB["Embeddings<br/>(bgeâ€‘largeâ€‘en)"]
    EMB --> VDB["Vector DB<br/>(QdrantÂ +Â cuVS)"]

    subgraph RAG["Retrievalâ€‘Augmented Generation"]
        QUERY["Task / Prompt"] --> QEMB["Embed Query"]
        QEMB --> VDB
        VDB --> RERANK["Crossâ€‘Encoder<br/>(ColBERTâ€‘v2)"]
        RERANK --> CONTEXT["Topâ€‘k Context"]
    end

    %% ========== GENERATION & SELFâ€‘CRITIQUE ==========
    CONTEXT --> LLM["LLM (Mistralâ€‘7B)<br/>+ Daily LoRA"]
    LLM --> SELF["Selfâ€‘RAG<br/>(reâ€‘retrieveÂ + critique)"]
    SELF --> DRAFT["Draft Tweet"]

    %% ========== REVIEW CHANNELS ==========
    DRAFT --> POREV{"Human Review?<br/>(early phase)"}
    DRAFT --> AIREV["AI Peer Review<br/>(raw GPTâ€‘4o)"]

    POREV -- "approve / minor" --> POST["Post to Twitter"]
    POREV -- "edit" --> EDIT["Manual Edit"]
    EDIT --> POST
    EDIT --> REWARD
    AIREV --> POST

    %% ========== FEEDBACK / REWARD ==========
    POST --> METRICS["Twitter Metrics<br/>(views â€¢ likes â€¢ reposts)"]
    POREV --> REWARD["Reward Model"]
    AIREV --> REWARD
    METRICS --> REWARD

    REWARD --> PPO["Policy Update<br/>(PPO via TRL)"]
    PPO --> TWAPI

    %% ========== CONTINUAL TUNING ==========
    CHUNK --> LORAFT["LoRA Fineâ€‘Tune<br/>(daily)"]
    LORAFT --> LLM

    %% ========== MONITORING ==========
    VDB -.-> DASH["Dash<br/>(Prom â€¢ RAGAS)"]
    LLM -. faithfulness .-> DASH
    POST -. brandâ€‘drift .-> DASH

```

### Abbreviation Glossary
| Term | Meaning / Role in Flow |
|------|------------------------|
| **Selfâ€‘RAG** | After drafting, the model *reâ€‘retrieves* supporting evidence from the vector DB and critiques or rewrites its own output for factual accuracy. |
| **LoRA (Lowâ€‘Rank Adaptation)** | Parameterâ€‘efficient fineâ€‘tuning that injects small rankâ€‘r weight matrices; only â‰ˆ2â€¯% of parameters are updated daily. |
| **PPO (Proximal Policy Optimisation)** | RL algorithm that maximises a clipped surrogate objective to keep policy updates stable. Implemented via the **TRL** (Transformer Reinforcement Learning) library from Huggingâ€¯Face. |
| **TRL** | Openâ€‘source Python library offering PPO, DPO and other RL algorithms tailored for transformer models. |

---

## 1. Dataâ€‘Quality Gate (Loop 1)

| Check | Method | Threshold | Action |
|-------|--------|-----------|--------|
| Language | `fastText` langâ€‘ID | nonâ€‘English? | filter |
| Toxicity | Google Perspective | > 0.80 | discard |
| Bot score | Botometerâ€‘Lite | top 10 % | discard |
| Perplexity band | GPTâ€‘2 PPL | keep 10â€‘90 % | keep |
| Engagement | likes + RT above median | âœ“ | priority |

Only messages passing **all** checks are chunked (256 tokens) and embedded.

---

## 2. Retrieval Loop (Loop 2)

* **Embedding model:** `bgeâ€‘largeâ€‘en` (or `textâ€‘embeddingâ€‘3â€‘small` if using OpenAI).  
* **Index:** HNSW < 10 M vectors; migrate to IVFâ€‘PQ + GPU search above that.  
* **Reâ€‘ranking:** ColBERTâ€‘v2 crossâ€‘encoder adds ~10â€“15 % precision.  
* **Key metrics:** precision@5, recall@10, contextâ€‘precision (RAGAS).  
* **Trigger:** reâ€‘index when precision@5 drops by 5 % WoW.

---

## 3. Generation Loop (Loop 3)

| Hyperâ€‘param | Value | Note |
|-------------|-------|------|
| Base model | Mistral 7B or Llamaâ€‘3 8B | openâ€‘weights |
| LoRA rank `r` | 16 | quality / VRAM tradeâ€‘off |
| Alpha | 2 Ã— r | scaling rule |
| LR (AdamW) | 1 Ã— 10â»â´ | tune first |
| Epochs | 1 | avoid overâ€‘fit |

Daily microâ€‘adapters are merged back every 4â€“6 weeks to prevent adapter sprawl.

**Selfâ€‘RAG + Reflexion**: model critiques and iterates once before final post; cuts hallucinations ~30 %.

---

## 4. Behaviour Loop (Loop 4)

* **Reward model inputs:** likes, retweets, CTR, follower delta (positive); toxicity, offâ€‘topic, low faithfulness (negative). Human and AI peer labels feed the same reward model used by PPO.  
* **Policy learner:** PPO updated weekly with 1â€‘step importance sampling.  
* **AI peer review:** a raw, nonâ€‘fineâ€‘tuned GPTâ€‘4o critiques every draft and supplies automatic feedback signals.  
* **Safety valve:** if faithfulness < 0.9 or toxicity > 0.6, autoâ€‘block posting & alert human.  
* **Implementation tip:** every manual *reject* or â€œmajor editâ€ is logged as a âˆ’1 reward; the next PPO cycle penalises that action pattern so the policy avoids it.

---

## 5. Daily Cadence (SGT)

| Time | Job | Loop |
|------|-----|------|
| 01:00 | Hydrate 24 h sources â†’ run quality gate | 1 |
| 02:00 | Embed & upsert vectors; rebuild ANN & reâ€‘rank index | 2 |
| 03:00 | LoRA fineâ€‘tune on new highâ€‘quality chunks | 3 |
| 09 â€“ 23 h | Agent reads, answers, posts (Selfâ€‘RAG active) | 2â€‘3 |
| 23:30 | Aggregate metrics â†’ reward logs â†’ PPO update | 4 |

---

## 6. Goldenâ€‘Signal Dashboard

| Signal | Threshold | Action |
|--------|-----------|--------|
| Retrieval precision@5 | â†“ > 5 % WoW | reâ€‘index |
| Faithfulness | < 0.90 | tighten filters / retrain generator |
| Hallucination rate | â†‘ WoW | increase Selfâ€‘RAG passes |
| p95 latency | > 2 s | scale index / lower k |
| Adapter count | > 8 | merge weights |

---

## 7. Knowledgeâ€‘Graph Hybrid

Immutable data (e.g., LBMA rules) lives in a Neo4j KG.  
Retriever first queries KG; if miss, fall back to vector DB, ensuring critical facts never drift.

---

## 8. Future Enhancements

**These modules are represented in the dashed â€œFuture Modulesâ€ node of the diagram.**  
* **Multiâ€‘lingual switch**: add languageâ€‘specific adapters & embeddings.  
* **Onâ€‘device summariser**: distil daily streams into trend reports.  
* **Synthetic user simulator**: generate interaction traces to preâ€‘train behaviour policy before launch.

---
